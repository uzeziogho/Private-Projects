{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9rRoiXd125Y3koqskC3jE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uzeziogho/Private-Projects/blob/main/Customer_churn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pcf4TWeSIjS-",
        "outputId": "acf3371b-2073-4b53-b7b3-cd6db83e087c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-e2376c3410f1>:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['TotalCharges'].fillna(0, inplace=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.77      0.80      1021\n",
            "           1       0.80      0.85      0.82      1049\n",
            "\n",
            "    accuracy                           0.81      2070\n",
            "   macro avg       0.82      0.81      0.81      2070\n",
            "weighted avg       0.81      0.81      0.81      2070\n",
            "\n",
            "Accuracy: 0.8135265700483092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [11:41:53] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "XGBoost:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.85      1021\n",
            "           1       0.85      0.86      0.85      1049\n",
            "\n",
            "    accuracy                           0.85      2070\n",
            "   macro avg       0.85      0.85      0.85      2070\n",
            "weighted avg       0.85      0.85      0.85      2070\n",
            "\n",
            "Accuracy: 0.8502415458937198\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\n",
            "Neural Network:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.83      1021\n",
            "           1       0.85      0.82      0.83      1049\n",
            "\n",
            "    accuracy                           0.83      2070\n",
            "   macro avg       0.83      0.83      0.83      2070\n",
            "weighted avg       0.83      0.83      0.83      2070\n",
            "\n",
            "Accuracy: 0.8309178743961353\n"
          ]
        }
      ],
      "source": [
        "# prompt: Generate code for Logistic regression, XGBoost and Neural Network with this dataset /content/WA_Fn-UseC_-Telco-Customer-Churn.csv managing imbalance\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import xgboost as xgb\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load the dataset\n",
        "try:\n",
        "  data = pd.read_csv('/content/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
        "except FileNotFoundError:\n",
        "  print(\"Error: File not found. Please make sure the file path is correct.\")\n",
        "  exit()\n",
        "\n",
        "\n",
        "# Preprocessing\n",
        "def preprocess_data(df):\n",
        "    df.drop('customerID', axis=1, inplace=True)\n",
        "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "    df['TotalCharges'].fillna(0, inplace=True)\n",
        "\n",
        "    categorical_cols = df.select_dtypes(include='object').columns\n",
        "    numerical_cols = df.select_dtypes(include=np.number).columns\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    for col in categorical_cols:\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "\n",
        "    X = df.drop('Churn', axis=1)\n",
        "    y = df['Churn']\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "X, y = preprocess_data(data)\n",
        "\n",
        "# Handle class imbalance using SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Logistic Regression\n",
        "logreg = LogisticRegression(solver = 'liblinear')\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred_logreg = logreg.predict(X_test)\n",
        "print(\"Logistic Regression:\")\n",
        "print(classification_report(y_test, y_pred_logreg))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_logreg))\n",
        "\n",
        "\n",
        "# XGBoost\n",
        "xgb_model = xgb.XGBClassifier(random_state=42, use_label_encoder =False, eval_metric='logloss')\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "print(\"\\nXGBoost:\")\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
        "\n",
        "\n",
        "# Neural Network\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0) # Verbose set to 0 to suppress output\n",
        "\n",
        "y_pred_nn = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "print(\"\\nNeural Network:\")\n",
        "print(classification_report(y_test, y_pred_nn))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_nn))"
      ]
    }
  ]
}