{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uzeziogho/Private-Projects/blob/main/Sustainability_%E2%80%93_Air_Quality_%26_Weather.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
        "\n",
        "# Step 2: Load dataset\n",
        "df = pd.read_csv(\"/content/AirQualityUCI.csv\", sep=';', decimal=',')  # decimal fix for numbers like \"7,8\"\n",
        "\n",
        "# Step 3: Handle missing values (-200 → NaN)\n",
        "df.replace(-200, np.nan, inplace=True)\n",
        "\n",
        "# Step 4: Select target and features\n",
        "target = \"CO(GT)\"  # pollutant to predict\n",
        "X = df.drop(columns=[target, \"Date\", \"Time\"], errors=\"ignore\")\n",
        "y = df[target]\n",
        "\n",
        "# Drop completely empty / unnamed columns\n",
        "X = X.loc[:, X.notna().any(axis=0)]\n",
        "\n",
        "# Step 5: Impute missing values\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "y = y.fillna(y.median())\n",
        "\n",
        "# Step 6: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 7: Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "X_scaled = scaler.fit_transform(X)   # full dataset for clustering\n",
        "\n",
        "# Step 8: Supervised Models\n",
        "models = {\n",
        "    \"Linear Regression\": LinearRegression(),\n",
        "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    \"XGBoost\": xgb.XGBRegressor(n_estimators=200, learning_rate=0.1, max_depth=6, random_state=42)\n",
        "}\n",
        "\n",
        "print(\"\\n=== SUPERVISED LEARNING (Regression) ===\")\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\n--- {name} ---\")\n",
        "    print(f\"RMSE: {rmse:.3f}\")\n",
        "    print(f\"MAE: {mae:.3f}\")\n",
        "    print(f\"R²: {r2:.3f}\")\n",
        "\n",
        "# Step 9: Unsupervised Clustering\n",
        "print(\"\\n=== UNSUPERVISED LEARNING (Clustering) ===\")\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "clusters = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Evaluate clustering\n",
        "sil_score = silhouette_score(X_scaled, clusters)\n",
        "ch_score = calinski_harabasz_score(X_scaled, clusters)\n",
        "\n",
        "print(f\"Silhouette Score: {sil_score:.3f}\")\n",
        "print(f\"Calinski-Harabasz Index: {ch_score:.3f}\")\n",
        "\n",
        "# Optional: Add cluster labels back to df for inspection\n",
        "df[\"Cluster\"] = clusters\n",
        "print(\"\\nCluster sample counts:\")\n",
        "print(df[\"Cluster\"].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ho95FBS-IPYr",
        "outputId": "558851dc-2d1c-4482-c40b-033ed9937d89"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== SUPERVISED LEARNING (Regression) ===\n",
            "\n",
            "--- Linear Regression ---\n",
            "RMSE: 0.555\n",
            "MAE: 0.372\n",
            "R²: 0.827\n",
            "\n",
            "--- Random Forest ---\n",
            "RMSE: 0.519\n",
            "MAE: 0.306\n",
            "R²: 0.849\n",
            "\n",
            "--- XGBoost ---\n",
            "RMSE: 0.493\n",
            "MAE: 0.308\n",
            "R²: 0.864\n",
            "\n",
            "=== UNSUPERVISED LEARNING (Clustering) ===\n",
            "Silhouette Score: 0.219\n",
            "Calinski-Harabasz Index: 3576.039\n",
            "\n",
            "Cluster sample counts:\n",
            "Cluster\n",
            "0    4036\n",
            "2    3147\n",
            "1    2288\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Supervised Learning (Regression)**\n",
        "\n",
        "You predicted CO(GT) (pollutant concentration) with three models.\n",
        "\n",
        "**Model\tRMSE\tMAE\tR²\tInterpretation**\n",
        "- Linear Regression\t0.555\t0.372\t0.827\tDecent linear fit. R² of 0.827 means ~83% of variance in CO levels is explained.\n",
        "- Random Forest\t0.519\t0.306\t0.849\tSlightly better than Linear Regression. Lower RMSE & MAE indicate smaller prediction errors.\n",
        "- XGBoost\t0.493\t0.308\t0.864\tBest model. R² of 0.864 indicates it explains ~86% of variance. RMSE is lowest, showing better prediction accuracy.\n",
        "\n",
        "**Takeaway:**\n",
        "\n",
        "All three models perform well, but XGBoost is the strongest, balancing low error and high explained variance.\n",
        "\n",
        "Random Forest also performs very well and is easier to interpret than XGBoost.\n",
        "\n",
        "Linear Regression is simpler but slightly less accurate.\n",
        "\n",
        "**Unsupervised Learning (Clustering)**\n",
        "\n",
        "You grouped the dataset into 3 clusters (pollution categories):\n",
        "\n",
        "Silhouette Score: 0.219\n",
        "\n",
        "Measures how well-separated clusters are.\n",
        "\n",
        "Score ranges from -1 to 1; closer to 1 = well-separated.\n",
        "\n",
        "0.219 is low-moderate, indicating clusters are somewhat overlapping.\n",
        "\n",
        "Calinski-Harabasz Index: 3576.039\n",
        "\n",
        "Higher is better; measures cluster compactness and separation.\n",
        "\n",
        "Decent value suggests clusters have internal cohesion and some separation.\n",
        "\n",
        "Cluster counts:\n",
        "\n",
        "- Cluster 0: 4036 samples\n",
        "- Cluster 2: 3147 samples\n",
        "- Cluster 1: 2288 samples\n",
        "\n",
        "\n",
        "Cluster 0 = largest group, possibly “moderate pollution”.\n",
        "\n",
        "Cluster 1 = smallest group, could correspond to “highest” or “lowest” pollution days.\n",
        "\n",
        "**Takeaway:**\n",
        "\n",
        "Clusters give a rough categorization of pollution levels, but some overlap exists.\n",
        "\n",
        "You can inspect the cluster averages for CO, NOx, NO2, etc., to label clusters as Low, Medium, High Pollution.\n",
        "\n",
        "**Overall Recommendation**\n",
        "\n",
        "- Supervised: Use XGBoost for final pollutant predictions.\n",
        "\n",
        "- Unsupervised: Use clusters as pollution categories for downstream analysis (e.g., alerting high pollution days).\n",
        "\n",
        "Optionally, combine regression predictions with cluster labels for deeper insights (e.g., which sensor readings predict high pollution days)."
      ],
      "metadata": {
        "id": "cn-F1HezQEFx"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhg65TkGpkVU1wAfVmcNCy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}